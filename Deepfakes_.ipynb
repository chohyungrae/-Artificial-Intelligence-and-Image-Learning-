{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deepfakes .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMopWqj/MvA8FCyS5SZpx+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chohyungrae/-Artificial-Intelligence-and-Image-Learning-/blob/master/Deepfakes_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWAXn9ciYwxP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8a0e3e2a-810b-4b8e-b0e0-fb57dac821d2"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR7NaqYiY21F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "80cc93d0-c265-4b94-fb66-8a86362aa5bf"
      },
      "source": [
        "!git clone --depth 1 https://github.com/deepfakes/faceswap.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'faceswap'...\n",
            "remote: Enumerating objects: 320, done.\u001b[K\n",
            "remote: Counting objects: 100% (320/320), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 320 (delta 67), reused 139 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (320/320), 782.44 KiB | 2.60 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxAAfCiBZE2z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "eea67313-5686-4e90-bfa9-33553f816ffa"
      },
      "source": [
        "%cd faceswap/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/faceswap\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaLyKUxoZLTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "outputId": "d702a281-fa31-433c-a79d-b6534b056a09"
      },
      "source": [
        "!python setup.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mINFO   \u001b[0m Running as Root/Admin\n",
            "\u001b[32mINFO   \u001b[0m The tool provides tips for installation\r\n",
            "        and installs required python packages\n",
            "\u001b[32mINFO   \u001b[0m Setup in Linux 4.19.112+\n",
            "\u001b[32mINFO   \u001b[0m Installed Python: 3.6.9 64bit\n",
            "\u001b[32mINFO   \u001b[0m Encoding: UTF-8\n",
            "\u001b[32mINFO   \u001b[0m Upgrading pip...\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 5.0MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installed pip: 19.3.1\n",
            "\u001b[32mINFO   \u001b[0m AMD Support: AMD GPU support is currently limited.\n",
            "        Nvidia Users MUST answer 'no' to this option.\n",
            "Enable AMD Support? [y/N] y\n",
            "\u001b[32mINFO   \u001b[0m AMD Support Enabled\n",
            "\u001b[32mINFO   \u001b[0m Faceswap config written to: /content/faceswap/config/.faceswap\n",
            "Please ensure your System Dependencies are met. Continue? [y/N] y\n",
            "\u001b[32mINFO   \u001b[0m Installing Required Python Packages. This may take some time...\n",
            "\u001b[32mINFO   \u001b[0m Installing tqdm>=4.42\n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.8 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing psutil>=5.7.0\n",
            "\u001b[K     |████████████████████████████████| 460 kB 8.3 MB/s \n",
            "\u001b[?25h  Building wheel for psutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[32mINFO   \u001b[0m Installing pillow>=7.0.0\n",
            "\u001b[32mINFO   \u001b[0m Installing toposort==1.5\n",
            "\u001b[32mINFO   \u001b[0m Installing fastcluster==1.1.26\n",
            "\u001b[K     |████████████████████████████████| 154 kB 8.7 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing imageio>=2.8.0\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 8.0 MB/s \n",
            "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
            "\n",
            "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
            "\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing imageio-ffmpeg>=0.4.2\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.2 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing ffmpy==0.2.3\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[32mINFO   \u001b[0m Installing Keras==2.2.4\n",
            "\u001b[K     |████████████████████████████████| 312 kB 9.5 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing plaidml-keras==0.6.4\n",
            "\u001b[K     |████████████████████████████████| 22.0 MB 100.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 312 kB 72.7 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing plaidml==0.6.4\n",
            "\u001b[K     |████████████████████████████████| 32.1 MB 1.2 MB/s \n",
            "\u001b[?25h\u001b[32mINFO   \u001b[0m Installing git+https://github.com/deepfakes/nvidia-ml-py3.git\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[32mINFO   \u001b[0m All python3 dependencies are met.\n",
            "        You are good to go.\n",
            "        \n",
            "        Enter:  'python faceswap.py -h' to see the options\n",
            "                'python faceswap.py gui' to launch the GUI\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww-Ev4oVcNqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "c163a98f-aca0-498e-fc23-13344f137a9f"
      },
      "source": [
        "!pip install gast==0.2.2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=b673fac3285bd3ca1c8aeca14817600c7e0cfaedc19cc6a24b845d3bd627213f\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
            "Successfully built gast\n",
            "Installing collected packages: gast\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgInILs9ZSPN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "e5245f4b-66a0-4013-e9f5-0f18913ad2f5"
      },
      "source": [
        "!python faceswap.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to AMD\n",
            "(Namespace(func=<function _bad_args at 0x7f6945c19598>),)\n",
            "usage: faceswap.py [-h] {extract,train,convert,gui} ...\n",
            "\n",
            "positional arguments:\n",
            "  {extract,train,convert,gui}\n",
            "    extract             Extract the faces from pictures\n",
            "    train               This command trains the model for the two faces A and\n",
            "                        B\n",
            "    convert             Convert a source image to a new one with the face\n",
            "                        swapped\n",
            "    gui                 Launch the Faceswap Graphical User Interface\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAYAXnTDa_8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b30938ff-f220-43ad-ad01-c50e92d15f2e"
      },
      "source": [
        "!python faceswap.py extract -h"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to AMD\n",
            "usage: faceswap.py extract [-h] [-C CONFIGFILE]\n",
            "                           [-L {INFO,VERBOSE,DEBUG,TRACE}] [-LF LOGFILE] -i\n",
            "                           INPUT_DIR -o OUTPUT_DIR [-al ALIGNMENTS_PATH]\n",
            "                           [-D {cv2-dnn,mtcnn,s3fd}] [-A {cv2-dnn,fan}]\n",
            "                           [-M {unet-dfl,vgg-clear,vgg-obstructed} [{unet-dfl,vgg-clear,vgg-obstructed} ...]]\n",
            "                           [-nm {none,clahe,hist,mean}] [-r ROTATE_IMAGES]\n",
            "                           [-min MIN_SIZE] [-n NFILTER [NFILTER ...]]\n",
            "                           [-f FILTER [FILTER ...]] [-l REF_THRESHOLD]\n",
            "                           [-een EXTRACT_EVERY_N] [-sz SIZE]\n",
            "                           [-si SAVE_INTERVAL] [-dl] [-s] [-sf] [-ssf]\n",
            "\n",
            "Extract the faces from pictures\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -C CONFIGFILE, --configfile CONFIGFILE\n",
            "                        Optionally overide the saved config with the path to a\n",
            "                        custom config file.\n",
            "  -L {INFO,VERBOSE,DEBUG,TRACE}, --loglevel {INFO,VERBOSE,DEBUG,TRACE}\n",
            "                        Log level. Stick with INFO or VERBOSE unless you need\n",
            "                        to file an error report. Be careful with TRACE as it\n",
            "                        will generate a lot of data\n",
            "  -LF LOGFILE, --logfile LOGFILE\n",
            "                        Path to store the logfile. Leave blank to store in the\n",
            "                        faceswap folder\n",
            "  -i INPUT_DIR, --input-dir INPUT_DIR\n",
            "                        Input directory or video. Either a directory\n",
            "                        containing the image files you wish to process or path\n",
            "                        to a video file. NB: This should be the source\n",
            "                        video/frames NOT the source faces.\n",
            "  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n",
            "                        Output directory. This is where the converted files\n",
            "                        will be saved.\n",
            "  -al ALIGNMENTS_PATH, --alignments ALIGNMENTS_PATH\n",
            "                        Optional path to an alignments file. Leave blank if\n",
            "                        the alignments file is at the default location.\n",
            "  -D {cv2-dnn,mtcnn,s3fd}, --detector {cv2-dnn,mtcnn,s3fd}\n",
            "                        Detector to use. Some of these have configurable\n",
            "                        settings in '/config/extract.ini' or 'Settings >\n",
            "                        Configure Extract 'Plugins':\n",
            "                          - cv2-dnn: A CPU only extractor which is the least\n",
            "                            reliable and least resource intensive. Use this if\n",
            "                            not using a GPU and time is important.\n",
            "                          - mtcnn: Good detector. Fast on CPU, faster on GPU.\n",
            "                            Uses fewer resources than other GPU detectors but\n",
            "                            can often return more false positives.\n",
            "                          - s3fd: Best detector. Fast on GPU, slow on CPU. Can\n",
            "                            detect more faces and fewer false positives than\n",
            "                            other GPU detectors, but is a lot more resource\n",
            "                            intensive.\n",
            "  -A {cv2-dnn,fan}, --aligner {cv2-dnn,fan}\n",
            "                        Aligner to use.\n",
            "                          - cv2-dnn: A CPU only landmark detector. Faster,\n",
            "                            less resource intensive, but less accurate. Only\n",
            "                            use this if not using a GPU and time is important.\n",
            "                          - fan: Best aligner. Fast on GPU, slow on CPU.\n",
            "  -M {unet-dfl,vgg-clear,vgg-obstructed} [{unet-dfl,vgg-clear,vgg-obstructed} ...], --masker {unet-dfl,vgg-clear,vgg-obstructed} [{unet-dfl,vgg-clear,vgg-obstructed} ...]\n",
            "                        Additional Masker(s) to use. The masks generated here\n",
            "                        will all take up GPU RAM. You can select none, one or\n",
            "                        multiple masks, but the extraction may take longer the\n",
            "                        more you select. NB: The Extended and Components\n",
            "                        (landmark based) masks are automatically generated on\n",
            "                        extraction.\n",
            "                          - vgg-clear: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces clear of\n",
            "                            obstructions. Profile faces and obstructions may\n",
            "                            result in sub-par performance.\n",
            "                          - vgg-obstructed: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces. The mask\n",
            "                            model has been specifically trained to recognize\n",
            "                            some facial obstructions (hands and eyeglasses).\n",
            "                            Profile faces may result in sub-par performance.\n",
            "                          - unet-dfl: Mask designed to provide smart\n",
            "                            segmentation of mostly frontal faces. The mask\n",
            "                            model has been trained by community members and\n",
            "                            will need testing for further description. Profile\n",
            "                            faces may result in sub-par performance.\n",
            "                        The auto generated masks are as follows:\n",
            "                          - components: Mask designed to provide facial\n",
            "                            segmentation based on the positioning of landmark\n",
            "                            locations. A convex hull is constructed around the\n",
            "                            exterior of the landmarks to create a mask.\n",
            "                          - extended: Mask designed to provide facial\n",
            "                            segmentation based on the positioning of landmark\n",
            "                            locations. A convex hull is constructed around the\n",
            "                            exterior of the landmarks and the mask is extended\n",
            "                            upwards onto the forehead.\n",
            "                        (eg: `-M unet-dfl vgg-clear`, `--masker vgg-\n",
            "                        obstructed`)\n",
            "  -nm {none,clahe,hist,mean}, --normalization {none,clahe,hist,mean}\n",
            "                        Performing normalization can help the aligner better\n",
            "                        align faces with difficult lighting conditions at an\n",
            "                        extraction speed cost. Different methods will yield\n",
            "                        different results on different sets. NB: This does not\n",
            "                        impact the output face, just the input to the aligner.\n",
            "                          - none: Don't perform normalization on the face.\n",
            "                          - clahe: Perform Contrast Limited Adaptive Histogram\n",
            "                            Equalization on the face.\n",
            "                          - hist: Equalize the histograms on the RGB channels.\n",
            "                          - mean: Normalize the face colors to the mean.\n",
            "  -r ROTATE_IMAGES, --rotate-images ROTATE_IMAGES\n",
            "                        If a face isn't found, rotate the images to try to\n",
            "                        find a face. Can find more faces at the cost of\n",
            "                        extraction speed. Pass in a single number to use\n",
            "                        increments of that size up to 360, or pass in a list\n",
            "                        of numbers to enumerate exactly what angles to check.\n",
            "  -min MIN_SIZE, --min-size MIN_SIZE\n",
            "                        Filters out faces detected below this size. Length, in\n",
            "                        pixels across the diagonal of the bounding box. Set to\n",
            "                        0 for off\n",
            "  -n NFILTER [NFILTER ...], --nfilter NFILTER [NFILTER ...]\n",
            "                        Optionally filter out people who you do not wish to\n",
            "                        process by passing in an image of that person. Should\n",
            "                        be a front portrait with a single person in the image.\n",
            "                        Multiple images can be added space separated. NB:\n",
            "                        Using face filter will significantly decrease\n",
            "                        extraction speed and its accuracy cannot be\n",
            "                        guaranteed.\n",
            "  -f FILTER [FILTER ...], --filter FILTER [FILTER ...]\n",
            "                        Optionally select people you wish to process by\n",
            "                        passing in an image of that person. Should be a front\n",
            "                        portrait with a single person in the image. Multiple\n",
            "                        images can be added space separated. NB: Using face\n",
            "                        filter will significantly decrease extraction speed\n",
            "                        and its accuracy cannot be guaranteed.\n",
            "  -l REF_THRESHOLD, --ref_threshold REF_THRESHOLD\n",
            "                        For use with the optional nfilter/filter files.\n",
            "                        Threshold for positive face recognition. Lower values\n",
            "                        are stricter. NB: Using face filter will significantly\n",
            "                        decrease extraction speed and its accuracy cannot be\n",
            "                        guaranteed.\n",
            "  -een EXTRACT_EVERY_N, --extract-every-n EXTRACT_EVERY_N\n",
            "                        Extract every 'nth' frame. This option will skip\n",
            "                        frames when extracting faces. For example a value of 1\n",
            "                        will extract faces from every frame, a value of 10\n",
            "                        will extract faces from every 10th frame.\n",
            "  -sz SIZE, --size SIZE\n",
            "                        The output size of extracted faces. Make sure that the\n",
            "                        model you intend to train supports your required size.\n",
            "                        This will only need to be changed for hi-res models.\n",
            "  -si SAVE_INTERVAL, --save-interval SAVE_INTERVAL\n",
            "                        Automatically save the alignments file after a set\n",
            "                        amount of frames. By default the alignments file is\n",
            "                        only saved at the end of the extraction process. NB:\n",
            "                        If extracting in 2 passes then the alignments file\n",
            "                        will only start to be saved out during the second\n",
            "                        pass. WARNING: Don't interrupt the script when writing\n",
            "                        the file because it might get corrupted. Set to 0 to\n",
            "                        turn off\n",
            "  -dl, --debug-landmarks\n",
            "                        Draw landmarks on the ouput faces for debugging\n",
            "                        purposes.\n",
            "  -s, --skip-existing   Skips frames that have already been extracted and\n",
            "                        exist in the alignments file\n",
            "  -sf, --skip-existing-faces\n",
            "                        Skip frames that already have detected faces in the\n",
            "                        alignments file\n",
            "  -ssf, --skip-saving-faces\n",
            "                        Skip saving out the face images\n",
            "\n",
            "Questions and feedback: https://faceswap.dev/forum\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdOIJ5fYbD97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "outputId": "1c0e7ed3-693f-45ad-f8bc-6a5dea5747a4"
      },
      "source": [
        "!python faceswap.py extract -i /content/faceswap/extract_video/aianchor640480size.mp4 -o extract_video"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to AMD\n",
            "08/11/2020 03:17:31 INFO     Log level set to: INFO\n",
            "08/11/2020 03:17:32 INFO     Setting up for PlaidML\n",
            "08/11/2020 03:17:32 INFO     Setting GPU to largest available supported device. If you want to override this selection, run `plaidml-setup` from the command line.\n",
            "08/11/2020 03:17:32 INFO     Using GPU: ['opencl_nvidia_tesla_t4.0', 'opencl_nvidia_tesla_t4.0']\n",
            "08/11/2020 03:17:32 INFO     Successfully set up for PlaidML\n",
            "08/11/2020 03:17:34 INFO     Output Directory: /content/faceswap/extract_video\n",
            "08/11/2020 03:17:34 INFO     Loading Detect from S3Fd plugin...\n",
            "Using plaidml.keras.backend backend.\n",
            "08/11/2020 03:17:35 INFO     Downloading model: 's3fd_keras' from: https://github.com/deepfakes-models/faceswap-models/releases/download/v1.11.1/s3fd_keras_v1.zip\n",
            "Downloading: 100% 11.1M/11.1M [00:00<00:00, 12.4MB/s]\n",
            "08/11/2020 03:17:36 INFO     Extracting: 's3fd_keras'\n",
            "Decompressing: 100% 85.9M/85.9M [00:00<00:00, 154MB/s]\n",
            "08/11/2020 03:17:37 INFO     Loading Align from Fan plugin...\n",
            "08/11/2020 03:17:37 INFO     Downloading model: 'face-alignment-network_2d4_keras' from: https://github.com/deepfakes-models/faceswap-models/releases/download/v1.9.1/face-alignment-network_2d4_keras_v1.zip\n",
            "Downloading: 100% 84.6M/84.6M [00:03<00:00, 28.2MB/s]\n",
            "08/11/2020 03:17:41 INFO     Extracting: 'face-alignment-network_2d4_keras'\n",
            "Decompressing: 100% 92.9M/92.9M [00:01<00:00, 91.1MB/s]\n",
            "08/11/2020 03:17:42 INFO     Loading Mask from Components plugin...\n",
            "08/11/2020 03:17:42 INFO     Loading Mask from Extended plugin...\n",
            "08/11/2020 03:17:42 INFO     Starting, this may take a while...\n",
            "08/11/2020 03:17:42 INFO     Initializing S3FD (Detect)...\n",
            "08/11/2020 03:17:42 INFO     Opening device \"opencl_nvidia_tesla_t4.0\"\n",
            "08/11/2020 03:17:45 INFO     Initialized S3FD (Detect) with batchsize of 4\n",
            "Running pass 1 of 4: Detect: 100% 194/194 [00:47<00:00,  4.08it/s]\n",
            "08/11/2020 03:18:33 INFO     Initializing FAN (Align)...\n",
            "08/11/2020 03:18:40 INFO     Analyzing Ops: 1434 of 3641 operations complete\n",
            "08/11/2020 03:18:42 INFO     Analyzing Ops: 3165 of 3641 operations complete\n",
            "08/11/2020 03:19:00 INFO     Initialized FAN (Align) with batchsize of 12\n",
            "08/11/2020 03:19:12 INFO     Analyzing Ops: 1398 of 3641 operations complete\n",
            "08/11/2020 03:19:14 INFO     Analyzing Ops: 3164 of 3641 operations complete\n",
            "Running pass 2 of 4: Align: 100% 194/194 [00:30<00:00,  6.31it/s]\n",
            "08/11/2020 03:19:31 INFO     Initializing Components (Mask)...\n",
            "08/11/2020 03:19:31 INFO     Initialized Components (Mask) with batchsize of 1\n",
            "Running pass 3 of 4: Mask: 100% 194/194 [00:01<00:00, 121.40it/s]\n",
            "08/11/2020 03:19:33 INFO     Initializing Extended (Mask)...\n",
            "08/11/2020 03:19:33 INFO     Initialized Extended (Mask) with batchsize of 1\n",
            "Running pass 4 of 4: Mask: 100% 194/194 [00:03<00:00, 53.95it/s]\n",
            "08/11/2020 03:19:36 INFO     Writing alignments to: '/content/faceswap/extract_video/aianchor640480size_alignments.fsa'\n",
            "08/11/2020 03:19:36 INFO     -------------------------\n",
            "08/11/2020 03:19:36 INFO     Images found:        194\n",
            "08/11/2020 03:19:36 INFO     Faces detected:      194\n",
            "08/11/2020 03:19:36 INFO     -------------------------\n",
            "08/11/2020 03:19:36 INFO     Process Succesfully Completed. Shutting Down...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye0CGi6ibP0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "52bdbe90-7f53-4c9a-9993-9a03a4603216"
      },
      "source": [
        "!python faceswap.py extract -i /content/faceswap/extract_video/EmmaWatson.mp4 -o extract_video2"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to AMD\n",
            "08/11/2020 03:30:29 INFO     Log level set to: INFO\n",
            "08/11/2020 03:30:29 INFO     Setting up for PlaidML\n",
            "08/11/2020 03:30:29 INFO     Setting GPU to largest available supported device. If you want to override this selection, run `plaidml-setup` from the command line.\n",
            "08/11/2020 03:30:29 INFO     Using GPU: ['opencl_nvidia_tesla_t4.0', 'opencl_nvidia_tesla_t4.0']\n",
            "08/11/2020 03:30:29 INFO     Successfully set up for PlaidML\n",
            "08/11/2020 03:30:31 INFO     Output Directory: /content/faceswap/extract_video2\n",
            "08/11/2020 03:30:31 INFO     Loading Detect from S3Fd plugin...\n",
            "Using plaidml.keras.backend backend.\n",
            "08/11/2020 03:30:32 INFO     Loading Align from Fan plugin...\n",
            "08/11/2020 03:30:32 INFO     Loading Mask from Components plugin...\n",
            "08/11/2020 03:30:32 INFO     Loading Mask from Extended plugin...\n",
            "08/11/2020 03:30:32 INFO     Starting, this may take a while...\n",
            "08/11/2020 03:30:32 INFO     Initializing S3FD (Detect)...\n",
            "08/11/2020 03:30:32 INFO     Opening device \"opencl_nvidia_tesla_t4.0\"\n",
            "08/11/2020 03:30:32 INFO     Initialized S3FD (Detect) with batchsize of 4\n",
            "Running pass 1 of 4: Detect: 100% 838/838 [02:06<00:00,  6.63it/s]\n",
            "08/11/2020 03:32:39 INFO     Initializing FAN (Align)...\n",
            "08/11/2020 03:32:43 INFO     Analyzing Ops: 936 of 3641 operations complete\n",
            "08/11/2020 03:32:45 INFO     Analyzing Ops: 2623 of 3641 operations complete\n",
            "08/11/2020 03:32:47 INFO     Initialized FAN (Align) with batchsize of 12\n",
            "08/11/2020 03:33:30 INFO     Analyzing Ops: 895 of 3641 operations complete\n",
            "08/11/2020 03:33:32 INFO     Analyzing Ops: 2583 of 3641 operations complete\n",
            "Running pass 2 of 4: Align: 100% 838/838 [00:47<00:00, 17.81it/s]\n",
            "08/11/2020 03:33:34 INFO     Initializing Components (Mask)...\n",
            "08/11/2020 03:33:34 INFO     Initialized Components (Mask) with batchsize of 1\n",
            "Running pass 3 of 4: Mask: 100% 838/838 [00:05<00:00, 157.34it/s]\n",
            "08/11/2020 03:33:39 INFO     Initializing Extended (Mask)...\n",
            "08/11/2020 03:33:39 INFO     Initialized Extended (Mask) with batchsize of 1\n",
            "Running pass 4 of 4: Mask: 100% 838/838 [00:12<00:00, 64.47it/s]\n",
            "08/11/2020 03:33:52 INFO     Writing alignments to: '/content/faceswap/extract_video/EmmaWatson_alignments.fsa'\n",
            "08/11/2020 03:33:53 INFO     -------------------------\n",
            "08/11/2020 03:33:53 INFO     Images found:        838\n",
            "08/11/2020 03:33:53 INFO     Faces detected:      838\n",
            "08/11/2020 03:33:53 INFO     -------------------------\n",
            "08/11/2020 03:33:53 INFO     Process Succesfully Completed. Shutting Down...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9QGjFddbS99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "7775512f-9ef0-4b4b-ad49-f87854062db6"
      },
      "source": [
        "#!python faceswap.py train -A extract_video -B /content/faceswap/extract_video/EmmaWatson.mp4 -m Model_data_new_videos\n",
        "!python faceswap.py train -A extract_video -B extract_video2 -m Model_data_new_videos"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to AMD\n",
            "08/11/2020 03:35:33 INFO     Log level set to: INFO\n",
            "08/11/2020 03:35:34 INFO     Setting up for PlaidML\n",
            "08/11/2020 03:35:34 INFO     Setting GPU to largest available supported device. If you want to override this selection, run `plaidml-setup` from the command line.\n",
            "08/11/2020 03:35:34 INFO     Using GPU: ['opencl_nvidia_tesla_t4.0', 'opencl_nvidia_tesla_t4.0']\n",
            "08/11/2020 03:35:34 INFO     Successfully set up for PlaidML\n",
            "Using plaidml.keras.backend backend.\n",
            "08/11/2020 03:35:36 INFO     Model A Directory: /content/faceswap/extract_video\n",
            "08/11/2020 03:35:36 INFO     Model B Directory: /content/faceswap/extract_video2\n",
            "08/11/2020 03:35:36 INFO     Training data directory: /content/faceswap/Model_data_new_videos\n",
            "08/11/2020 03:35:36 INFO     ===================================================\n",
            "08/11/2020 03:35:36 INFO       Starting\n",
            "08/11/2020 03:35:36 INFO       Press 'ENTER' to save and quit\n",
            "08/11/2020 03:35:36 INFO       Press 'S' to save model weights immediately\n",
            "08/11/2020 03:35:36 INFO     ===================================================\n",
            "08/11/2020 03:35:37 INFO     Loading data, this may take a while...\n",
            "08/11/2020 03:35:37 INFO     Loading Model from Original plugin...\n",
            "08/11/2020 03:35:37 INFO     No existing state file found. Generating.\n",
            "08/11/2020 03:35:37 INFO     Opening device \"opencl_nvidia_tesla_t4.0\"\n",
            "08/11/2020 03:35:41 INFO     Creating new 'original' model in folder: '/content/faceswap/Model_data_new_videos'\n",
            "08/11/2020 03:35:41 INFO     Loading Trainer from Original plugin...\n",
            "08/11/2020 03:35:42 INFO     Enabled TensorBoard Logging\n",
            "08/11/2020 03:35:47 INFO     Analyzing Ops: 157 of 520 operations complete\n",
            "08/11/2020 03:36:02 INFO     Analyzing Ops: 162 of 520 operations complete\n",
            "[03:36:13] [#00001] Loss A: 0.22260, Loss B: 0.20688\n",
            "08/11/2020 03:36:13 INFO     Backing up models...\n",
            "08/11/2020 03:36:14 INFO     [Saved models] - Average since last save: face_loss_A: 0.22260, face_loss_B: 0.20688\n",
            "[03:38:29] [#00101] Loss A: 0.09050, Loss B: 0.09305\n",
            "08/11/2020 03:38:29 INFO     Backing up models...\n",
            "08/11/2020 03:38:30 INFO     [Saved models] - Average since last save: face_loss_A: 0.12464, face_loss_B: 0.12326\n",
            "[03:40:46] [#00201] Loss A: 0.08279, Loss B: 0.07594\n",
            "08/11/2020 03:40:46 INFO     Backing up models...\n",
            "08/11/2020 03:40:47 INFO     [Saved models] - Average since last save: face_loss_A: 0.08485, face_loss_B: 0.08262\n",
            "[03:43:02] [#00301] Loss A: 0.06924, Loss B: 0.06640\n",
            "08/11/2020 03:43:02 INFO     Backing up models...\n",
            "08/11/2020 03:43:03 INFO     [Saved models] - Average since last save: face_loss_A: 0.07061, face_loss_B: 0.07246\n",
            "[03:45:19] [#00401] Loss A: 0.05685, Loss B: 0.06662\n",
            "08/11/2020 03:45:19 INFO     Backing up models...\n",
            "08/11/2020 03:45:20 INFO     [Saved models] - Average since last save: face_loss_A: 0.06290, face_loss_B: 0.06637\n",
            "[03:47:35] [#00501] Loss A: 0.05567, Loss B: 0.07100\n",
            "08/11/2020 03:47:35 INFO     Backing up models...\n",
            "08/11/2020 03:47:37 INFO     [Saved models] - Average since last save: face_loss_A: 0.05734, face_loss_B: 0.06266\n",
            "[03:49:52] [#00601] Loss A: 0.04986, Loss B: 0.05532\n",
            "08/11/2020 03:49:52 INFO     Backing up models...\n",
            "08/11/2020 03:49:53 INFO     [Saved models] - Average since last save: face_loss_A: 0.05486, face_loss_B: 0.06009\n",
            "[03:52:09] [#00701] Loss A: 0.05173, Loss B: 0.06088\n",
            "08/11/2020 03:52:09 INFO     Backing up models...\n",
            "08/11/2020 03:52:10 INFO     [Saved models] - Average since last save: face_loss_A: 0.05173, face_loss_B: 0.05758\n",
            "[03:53:22] [#00754] Loss A: 0.04925, Loss B: 0.05802"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igxs_fGTbWPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "a72b7961-13d3-4010-e18a-b7f29689a29a"
      },
      "source": [
        "!python faceswap.py convert -i/content/faceswap/extract_video/aianchor640480size.mp4 -o outpute_dir_video -m \"/content/faceswap/Model_data_new_videos\" -w ffmpeg"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting Faceswap backend to AMD\n",
            "08/11/2020 03:53:27 INFO     Log level set to: INFO\n",
            "08/11/2020 03:53:27 INFO     Setting up for PlaidML\n",
            "08/11/2020 03:53:28 INFO     Setting GPU to largest available supported device. If you want to override this selection, run `plaidml-setup` from the command line.\n",
            "08/11/2020 03:53:28 INFO     Using GPU: ['opencl_nvidia_tesla_t4.0', 'opencl_nvidia_tesla_t4.0']\n",
            "08/11/2020 03:53:28 INFO     Successfully set up for PlaidML\n",
            "Using plaidml.keras.backend backend.\n",
            "08/11/2020 03:53:30 INFO     Reading alignments from: '/content/faceswap/extract_video/aianchor640480size_alignments.fsa'\n",
            "08/11/2020 03:53:30 INFO     Loading Writer from Ffmpeg plugin...\n",
            "08/11/2020 03:53:30 INFO     Loading Model from Original plugin...\n",
            "08/11/2020 03:53:30 INFO     Using configuration saved in state file\n",
            "08/11/2020 03:53:30 INFO     Opening device \"opencl_nvidia_tesla_t4.0\"\n",
            "08/11/2020 03:53:31 INFO     Opening device \"opencl_nvidia_tesla_t4.0\"\n",
            "08/11/2020 03:53:33 INFO     Loaded model from disk: '/content/faceswap/Model_data_new_videos'\n",
            "08/11/2020 03:53:33 INFO     Loading Mask from Box_Blend plugin...\n",
            "08/11/2020 03:53:33 INFO     Loading Mask from Mask_Blend plugin...\n",
            "08/11/2020 03:53:33 INFO     Loading Color from Avg_Color plugin...\n",
            "08/11/2020 03:53:36 INFO     Outputting to: '/content/faceswap/outpute_dir_video/aianchor640480size_converted.mp4'\n",
            "Converting: 100% 194/194 [00:16<00:00, 11.82it/s]\n",
            "08/11/2020 03:53:46 INFO     Muxing Audio...\n",
            "08/11/2020 03:53:47 INFO     -------------------------\n",
            "08/11/2020 03:53:47 INFO     Images found:        194\n",
            "08/11/2020 03:53:47 INFO     Faces detected:      194\n",
            "08/11/2020 03:53:47 INFO     -------------------------\n",
            "08/11/2020 03:53:47 INFO     Process Succesfully Completed. Shutting Down...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gdocjoybZjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}